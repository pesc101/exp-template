# conf/model/llama.yaml
model_name: "meta-llama/Llama-3.2-1B-Instruct"
model_name_short: "llama3.2-1B-instruct"
gpu_memory_utilization: 0.2
temperature: 0
max_tokens: 1000
top_p: 0.95
max_model_len: 40000
